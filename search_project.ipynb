{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "mystem = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(input_text, del_stopwords=True, del_digit=True):\n",
    "    \"\"\"\n",
    "    :input: raw text\n",
    "    1. lowercase, del punctuation, tokenize\n",
    "    2. normal form\n",
    "    3. del stopwords\n",
    "    4. del digits\n",
    "    :return: lemmas\n",
    "    \"\"\"\n",
    "    russian_stopwords = set(stopwords.words('russian'))\n",
    "    words = [x.lower().strip(punctuation+'»«–…') for x in word_tokenize(input_text)]\n",
    "    lemmas = [mystem.lemmatize(x)[0] for x in words if x]\n",
    "\n",
    "    lemmas_arr = []\n",
    "    for lemma in lemmas:\n",
    "        if del_stopwords:\n",
    "            if lemma in russian_stopwords:\n",
    "                continue\n",
    "        if del_digit:\n",
    "            if lemma.isdigit():\n",
    "                continue\n",
    "        lemmas_arr.append(lemma)\n",
    "    return lemmas_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def file_split(file):\n",
    "    with open(file, 'r', encoding = 'utf-8') as f:\n",
    "        text = f.read().split('\\n')\n",
    "        info = {}\n",
    "        info['name'] = text[0]\n",
    "        info['price'] = text[1]\n",
    "        info['date'] = text[2]\n",
    "        info['view'] = text[3]\n",
    "        info['author'] = text[4]\n",
    "        info['adress'] = text[5]\n",
    "        info['text'] = ' '.join(text[6:])\n",
    "        return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import math\n",
    "from math import log\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_information(path, kind):\n",
    "    \"\"\"\n",
    "    Create inverted index by input doc collection\n",
    "    Get essential information from input doc collection\n",
    "    :return: inverted index, \n",
    "    \"\"\"\n",
    "    dictionary = {}\n",
    "    files_len = {}\n",
    "    for file in os.listdir(path):\n",
    "        try:\n",
    "            info = file_split(path+'/'+file)\n",
    "            if kind == 'date' or kind == 'price':\n",
    "                words = preprocessing(info[kind], del_digit=False)\n",
    "            else:\n",
    "                words = preprocessing(info[kind])\n",
    "                files_len[file] = len(words)\n",
    "                counter = Counter(words)\n",
    "                for word in counter:\n",
    "                    if word in dictionary:\n",
    "                        dictionary[word][file] = counter[word]\n",
    "                    else:\n",
    "                        dictionary[word] = {}\n",
    "                        dictionary[word][file] = counter[word]\n",
    "        except:\n",
    "            pass\n",
    "    return files_len, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kinds = ['name', 'author', 'date', 'adress', 'price', 'text', 'view']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = {}\n",
    "files_len = {}\n",
    "for kind in kinds:\n",
    "    files_len[kind], index[kind] = get_information('avito', kind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('index.txt', 'r', encoding='utf-8') as f:\n",
    "    index = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('files_len.txt', 'r', encoding='utf-8') as f:\n",
    "    files_len = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k1 = 2.0\n",
    "b = 0.75\n",
    "\n",
    "def score_BM25(qf, dl, avgdl, k1, b, N, n) -> float:\n",
    "    \"\"\"\n",
    "    Compute similarity score between search query and documents from collection\n",
    "    :return: score\n",
    "    \"\"\"\n",
    "    score = math.log((N-n+0.5)/(n+0.5)) * (k1+1)*qf/(qf+k1*(1-b+b*(dl/avgdl)))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_sim(word, index, files_len):\n",
    "    \"\"\"\n",
    "    Compute similarity score between search query and documents from collection\n",
    "    :return: score\n",
    "    \"\"\"\n",
    "    N = len(files_len)\n",
    "    avgdl = sum(files_len.values())/N\n",
    "    if word in index:\n",
    "        n = len(index[word])\n",
    "        result = {}\n",
    "        for file in index[word]:\n",
    "            qf = index[word][file]\n",
    "            score = score_BM25(qf, files_len[file], avgdl, k1, b, N, n)\n",
    "            result[file] = score\n",
    "        return result\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "\n",
    "def get_search_result(inquiry, kind):\n",
    "    \"\"\"\n",
    "    Compute sim score between search query and all documents in collection\n",
    "    :return: list of files\n",
    "    \"\"\"\n",
    "    global index, files_len\n",
    "    score = defaultdict(int)\n",
    "    if kind == 'date' or kind == 'price':\n",
    "        words = preprocessing(inquiry, del_digit=False)\n",
    "    else:\n",
    "        words = preprocessing(inquiry)\n",
    "    for word in words:\n",
    "        result = compute_sim(word, index[kind], files_len[kind])\n",
    "        for file in result:\n",
    "            score[file] += result[file]  \n",
    "        \n",
    "    return sorted(score, key=score.get, reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "from flask import render_template, request\n",
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load('/users/kata/araneum_none_fasttextcbow_300_5_2018/araneum_none_fasttextcbow_300_5_2018.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_w2v_vectors(text, model): \n",
    "    \"\"\"\n",
    "    Получает вектор документа\n",
    "    \"\"\"\n",
    "    n = 0\n",
    "    vector = [0] * 300\n",
    "    lemmas = preprocessing(text)\n",
    "    for lemma in lemmas:\n",
    "        try:\n",
    "            vector += model.wv[lemma]\n",
    "            n += 1\n",
    "        except:\n",
    "            None\n",
    "    if n != 0:\n",
    "        vector = vector / n\n",
    "    return vector\n",
    "\n",
    "def save_w2v_base(path, model):\n",
    "    \"\"\"Индексирует всю базу для поиска через word2vec\"\"\"\n",
    "    w2v_base = []\n",
    "    for file in os.listdir(path=path)[1:]:\n",
    "        info = file_split(path+'/'+file)\n",
    "        vec = {}\n",
    "        vec['id'] = file\n",
    "        vec['text'] = info['text']\n",
    "        vec['vector'] = get_w2v_vectors(info['text'], model)\n",
    "        w2v_base.append(vec)\n",
    "            \n",
    "\n",
    "    return w2v_base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_base = save_w2v_base('avito', w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tagged_data(path):\n",
    "    tagged_data = []\n",
    "    i = 0\n",
    "    for file in os.listdir(path=path)[1:]:\n",
    "        try:\n",
    "            info = file_split(path+'/'+file)\n",
    "            data = preprocessing(info['text'], del_stopwords=False)\n",
    "            tagged_data.append(TaggedDocument(words=data, tags=[i]))\n",
    "            i+=1\n",
    "        except:\n",
    "            None\n",
    "    return tagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_doc2vec(tagged_data):\n",
    "    model = Doc2Vec(vector_size=100, min_count=5, alpha=0.025, min_alpha=0.025, epochs=100, workers=2, dm=1)\n",
    "    model.build_vocab(tagged_data)\n",
    "    model.random.seed(12345)\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_d2v_vectors(text, model):\n",
    "    \"\"\"Получает вектор документа\"\"\"\n",
    "    return model.infer_vector(text)\n",
    "\n",
    "def save_d2v_base(path, model):\n",
    "    \"\"\"Индексирует всю базу для поиска через doc2vec\"\"\"\n",
    "    d2v_base = []\n",
    "    for file in os.listdir(path=path)[1:]:\n",
    "        info = file_split(path+'/'+file)\n",
    "        vec = {}\n",
    "        vec['id'] = file\n",
    "        vec['text'] = info['text']\n",
    "        vec['vector'] = get_d2v_vectors(info['text'], model)\n",
    "        d2v_base.append(vec)\n",
    "    return d2v_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2w_model = train_doc2vec(get_tagged_data('avito'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_base = save_d2v_base('avito', d2w_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import matutils\n",
    "import numpy as np \n",
    "\n",
    "def similarity(v1, v2):\n",
    "    v1_norm = matutils.unitvec(np.array(v1))\n",
    "    v2_norm = matutils.unitvec(np.array(v2))\n",
    "    return np.dot(v1_norm, v2_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_w2v(inquiry):\n",
    "    global w2v_base, w2v_model\n",
    "    v1 = get_w2v_vectors(inquiry, w2v_model)\n",
    "    score = defaultdict(int)\n",
    "    for vec in w2v_base:\n",
    "        v2 = vec['vector']\n",
    "        sim = similarity(v1, v2)\n",
    "        score[vec['text']] = sim\n",
    "    return sorted(score, key=score.get, reverse = True)[:10]\n",
    "        \n",
    "\n",
    "def search_d2v(inquiry):\n",
    "    global d2v_base, d2w_model\n",
    "    v1 = get_d2v_vectors(inquiry, d2w_model)\n",
    "    score = defaultdict(int)\n",
    "    for vec in d2v_base:\n",
    "        v2 = vec['vector']\n",
    "        sim = similarity(v1, v2)\n",
    "        score[vec['text']] = sim\n",
    "    return sorted(score, key=score.get, reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kata/anaconda/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['letopisi_1148732967.txt',\n",
       " 'vvedenie_v_professiyu_psiholog_975252123.txt',\n",
       " 'skazka_kolobok_kniga-panorama_1413374738.txt',\n",
       " 'herluf_bidstrup_1505746233.txt',\n",
       " 'duel_s_odnim_pistoletom_smirenskiy_mihail_1175100792.txt',\n",
       " 'fables_9sons_of_empire_1705480580.txt',\n",
       " 'shkola_risovaniya_1635959296.txt',\n",
       " 'knigizhzl_1711397912.txt',\n",
       " 'karmannyy_atlas_sssr_1083085053.txt',\n",
       " 'kniga_1688439272.txt']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_d2v('книга')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kata/anaconda/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['voronin_neznanskiy_shitov_topol_890410197.txt',\n",
       " 'aleksandr_zorich_6_knig_985215302.txt',\n",
       " 'aferisty_a_malyugin_1161825982.txt',\n",
       " 'detskie_knigi_1718632671.txt',\n",
       " 'evropeyskoe_iskusstvo_xlx_veka_1140898144.txt',\n",
       " 'georgiy_sytin_1422733275.txt',\n",
       " 'kniga_bud_yasnym_den_a_korneev_1322027375.txt',\n",
       " 'kniga_chernaya_zhemchuzhina_viktor_vazhdaev_1610234720.txt',\n",
       " 'kniga_timka_boris_raevskiy_1657827033.txt',\n",
       " 'knigi_1425913755.txt']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_w2v('книга')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [23/Oct/2018 15:20:44] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Oct/2018 15:20:53] \"GET /poisk2 HTTP/1.1\" 200 -\n",
      "/Users/kata/anaconda/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n",
      "127.0.0.1 - - [23/Oct/2018 15:21:04] \"GET /poisk2?word=%D0%BB%D0%B5%D1%80%D0%BC%D0%BE%D0%BD%D1%82%D0%BE%D0%B2&kind=text&search_method=word2vec HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "@app.route('/poisk2')\n",
    "def poisk4():\n",
    "    if request.args:\n",
    "        inquiry = request.args['word']\n",
    "        kind = request.args['kind']\n",
    "        search_method = request.args['search_method']\n",
    "        if search_method == 'inverted_index':\n",
    "            b = get_search_result(inquiry, kind)\n",
    "        elif search_method == 'word2vec':\n",
    "            b = search_w2v(inquiry)\n",
    "        else:\n",
    "            b = search_d2v(inquiry)\n",
    "        if b == []:\n",
    "            b.append('Извините, по вашему запросу ничего не найдено. Попробуйте ещё!')\n",
    "        return render_template('result1.html', b=b)\n",
    "    return render_template('search1.html')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
